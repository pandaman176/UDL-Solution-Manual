\documentclass{article}
\usepackage{amsmath} % For mathematical symbols and environments
\usepackage{amsfonts} % For additional fonts
\usepackage{graphicx} % For including graphics

\begin{document}

\section*{Problems}

    \subsection*{Problem 2.1}
    To walk "downhill" on the loss function (equation 2.5), we measure its gradient with respect to the parameters $\phi_0$ and $\phi_1$. Calculate expressions for the slopes $\frac{\partial L}{\partial \phi_0}$ and $\frac{\partial L}{\partial \phi_1}$.

        \vspace{1cm}
        \textbf{Solution:}
        \begin{align*}
            % Add solution here
        \end{align*}

    \subsection*{Problem 2.2}
    Show that we can find the minimum of the loss function in closed form by setting the expression for the derivatives from problem 2.1 to zero and solving for $\phi_0$ and $\phi_1$. Note that this works for linear regression but not for more complex models; this is why we use iterative model fitting methods like gradient descent (figure 2.4).

        \vspace{1cm}
        \textbf{Solution:}
        \begin{align*}
            % Add solution here
        \end{align*}

    \subsection*{Problem 2.3*}
    Consider reformulating linear regression as a generative model, so we have $g[y, \phi] = \phi_0 + \phi_1 y$. What is the new loss function? Find an expression for the inverse function $y = g^{-1}[x, \phi]$ that we would use to perform inference. Will this model make the same predictions as the discriminative version for a given training dataset $\{(x_i, y_i)\}$? One way to establish this is to write code that fits a line to three data points using both methods and see if the result is the same.

        \vspace{1cm}
        \textbf{Solution:}
        \begin{align*}
            % Add solution here
        \end{align*}


\end{document}